# 十八、其他工具和服务

在这一章中，我们将会看到各种各样的工具、库和服务。这些通常会贯穿我们在前面章节中看到的所有层，所以我们在本章中会单独讨论它们。

并非所有的工具总是必不可少的；我们在下面的列表中包括了基本的和有用的工具。重要的是从小处着手，通过添加更复杂、更先进的工具来提高生产力，从而逐步发展——在这一发展过程中，了解本章中各种类别的工具、库和服务将会很有帮助。

## 发展环境

数据科学家和数据工程师使用 dev 环境来编写数据科学过程中从数据捕获到机器学习的所有步骤的代码。如果你只是从一两个数据科学家开始，并且你能够以 CSV 文件的形式向他们提供数据，他们可以简单地在他们各自的机器上使用他们选择的 IDE <sup>[1](#Fn1)</sup> 进行分析，例如 Spyder 或 RStudio。但是，数据科学团队通常在高度协作的环境中工作，在像*笔记本*这样的环境中编码，这些笔记本位于一个共享位置，以便与团队的其他成员进行讨论。理想情况下，笔记本还应该支持多名数据科学家同时进行协作编辑。

Jupyter 笔记本是数据科学家最常见的环境。其他流行的环境有 Databricks、Sagemaker Studio、JupyterHub 和 Zeppelin。其中一些允许在单个笔记本中混合使用 R/Python/SQL，如果您的数据科学团队混合使用这些技能，这将非常有用。

开发环境还应该允许数据科学团队注册通用的标准库，以确保整个团队使用不同库的相同版本。

在第 [19](19.html) 章中，我们将会看到一个开发环境是如何与所有其他组件结合使用的。

## 实验注册

实验注册表是存储数据科学团队执行的所有实验的地方。实验注册中心需要支持以下内容:

*   存储参数、图表、指标等。，其中包含一个实验的细节。

*   使用标签/关键字等搜索实验。

*   根据参数和结果比较多个实验。

*   维护用于执行实验的笔记本/源代码版本的链接。这对于追踪血统和能够复制实验是至关重要的。

*   将模型存储在模型注册表中<sup>[2](#Fn2)</sup>；模型注册表中的每个条目都将链接到原始实验，以允许将模型追溯到实验以及相应的源代码和数据。

MLFlow 是一个流行的开源实验注册表，它提供了上述功能。一些开发环境捆绑了一个实验注册表作为其产品的一部分。 <sup>[3](#Fn3)</sup>

实验注册中心充当组织中数据科学团队所有活动的中央长期存储库。因此，保持数据科学团队的业务连续性至关重要。通过存储所有实验和谱系的历史，它还通过允许实验的回顾和再现来帮助确保必要的科学严谨性。

## 计算基础设施

在数据科学团队的运营中，计算资源有三大用途:

*   提供服务器来托管开发环境。例如，这包括一个 JupyterHub 服务器，它可以根据团队的需求和规模进行扩展。在云托管的环境中，如 Databricks，这可能由服务提供商负责。

*   为执行数据科学团队编写的笔记本/脚本提供计算资源。这包括任何可扩展的集群(如 Spark 集群)、GPU 机器等。，用于数据科学过程中的数据准备和机器学习步骤。

*   为托管可视化工具 <sup>[4](#Fn4)</sup> (例如 Tableau)和 SQL 查询引擎 <sup>[5](#Fn5)</sup> (例如 Presto)提供服务器。

计算资源可以属于以下任何类别:

*   单独的机器(本地的物理机器，或者像 AWS Ec2 这样的云服务)。通常用于
    *   数据科学指数据可以放在一台机器上

    *   托管服务器或第三方工具

*   Spark 等计算集群(内部部署，或 Databricks 或 Amazon EMR 等服务)

*   GPU 机器(单个或多个 GPU)

*   使用 Horovod 等框架的 GPU 集群

*   Amazon ECS 等容器托管服务。 <sup>[6](#Fn6)</sup> 通常用于打包为 Docker 容器或可使用 Docker 部署的工具/库的复杂 ML 作业

## AutoML

人工智能的动机之一是自动化大量由人类完成的重复性工作。如果我们的目标是让数据科学家的工作自动化，会怎么样？这是 AutoML 的愿景。

从原始意义上来说，数据科学家是做什么的？他们仅仅尝试各种数据准备操作，并尝试具有不同参数设置的各种模型，以达到最佳模型。如果一个算法只是简单地强行尝试所有流行的模型，如 XGBoost、LinearRegression、NeuralNetworks、 <sup>[7](#Fn7)</sup> 等等，会怎么样？我们已经开始看到 AutoML 的基础了。

当然，运行各种模型和神经网络架构并不是一件容易的事情。AutoML 解决方案采用几种先进的技术和启发式方法来优化搜索，以获得一个好的模型。

### AutoML 的目的

AutoML 有两个主要用途:

*   民主化数据科学:AutoML 服务允许，比如说，软件工程师在不了解数据科学的情况下使用数据科学的力量。在某种程度上，AutoML 对于现在的数据科学，就像 SQL 对于 20 世纪 80 年代以来的数据处理一样。就像 SQL 允许编程技能较低的人以声明方式查询数据一样，AutoML 开始允许工程师通过自动化数据科学过程中的许多具有挑战性的步骤来将科学方法应用于数据。

*   自动化数据科学家的重复任务:数据科学家需要做大量的手动调优和微小变化的重复实验。他们还经常执行*模型扫描*，即使用各种候选建模方法进行实验，以缩小有前途的继续研究的范围。在所有这些领域，AutoML 可以通过自动化这些重复的任务来帮助数据科学家。从这个意义上说，AutoML 对于数据科学家就像 SQL 对于软件工程师一样——软件工程师不需要声明性语言；他们可以编写查询数据所需的复杂程序。但是尽管如此，SQL 通过自动化高效查询多个表中的数据等重复性编程任务，使他们的工作变得更加容易。

### 小心翼翼

在使用 AutoML 时小心谨慎是很重要的。如果团队中有人正在使用 AutoML，那么他们知道如何评估模型性能是很重要的，这样他们就可以确定结果模型是否足够好。他们还需要完全理解输入 AutoML 的数据和问题本身的表述(哪些是目标变量，哪些是特征，等等。).拥有这些技能的人现在被称为 ML 工程师——更多细节请参考第 [21](21.html) 章。

AutoML 并没有减少对领域理解的需求——特别是在识别正确的特性和一些特定领域的数据准备步骤时。例如，您的目标可能是预测某个地区各个加油站的燃料库存何时需要补充，这样您就可以优化从主码头到加油站的燃料配送。在这种情况下，乍一看，我们似乎需要预测一个加油站的库存。但库存通常不会有一个明确的模式——更有可能有一个模式的是加油站的销售。销售可能取决于一周中的某一天、某一天是否是假日、天气等等。一旦我们有了一个可以预测销售的模型，我们就可以根据当前的库存和未来的销售来推断未来的库存。从未来的库存，我们可以确定什么时候库存会低，需要补货。即使一个人正在使用 AutoML，这种问题公式化仍然需要由数据科学家或 ML 工程师来完成。

虽然 AutoML 可以完成一些数据准备方面的工作，如标准化或缺失值处理，但 AutoML 的主要重点是自动化机器学习步骤。人类仍然需要执行数据科学过程的其他步骤。

### 工具和服务

AutoML 工具和服务有多种风格。我们在这里提到一个分类:

*   开源库，如
    *   Auto-sklearn，它构建在 scikit-learn 之上，提供 AutoML 功能

    *   亚马逊开源的自动增长

*   亚马逊 Sagemaker Autopilot、谷歌云 AutoML、Azure 机器学习等云服务都支持 AutoML。除了机器学习步骤，它们还可以在不同程度上支持数据准备步骤，例如，文本、图像等的特征化。

一些工具可能以自动化的方式支持超参数调优，并将其称为 AutoML。由于术语 AutoML 在市场上没有明确的定义，因此检查声称提供 AutoML 的工具或服务的详细功能非常重要。

## 多模态预测分析和机器学习

多模态预测分析和机器学习(PAML) <sup>[8](#Fn8)</sup> 工具提供高级功能，以支持大数据规模的端到端数据科学流程。这包括

*   能够在可视化的无代码 UI 中定义数据准备和机器学习工作流。

*   数据科学家和数据工程师将自定义代码插入可视化工作流的挂钩。

*   为喜欢编码的数据科学家集成笔记本。

*   能够与其他团队成员分享实验及其结果。这可能还包括各种角色，如编辑、审阅者等。

*   跨实验跟踪数据谱系，以支持审核、实验的可重复性和可追溯性。

*   无缝地将模型部署到生产环境中，而无需编写额外的代码。

*   监控生产中的模型，AB 测试等。

一方面，多模式 PAML 工具在开箱即用方面有很大不同，另一方面，在多大程度上允许定制方面也有很大不同。

SAS 可视化数据挖掘和机器学习(VDMML)结合 SAS Viya 产品套件是目前市场上最全面的 PAML 工具之一，支持前面提到的所有功能。

KNIME Analytics Platform 是一个开源工具，支持为整个数据科学流程可视化地定义数据科学工作流。KNIME Server 是一个付费的企业工具，支持工作流的协作和交互执行。

如果有市民数据科学家 <sup>[9](#Fn9)</sup> 和数据分析师与数据科学团队一起工作，多模态 PAML 工具会特别有用。

根据我们的经验，虽然多模式 PAML 工具可以提高大多数常见实验流程的效率，但当涉及到实验设计中高度复杂的变化时，它们往往会成为一个障碍。采用多模态 PAML 工具的决定不容轻率——需要考虑各种因素，以确保它们提高生产力和协作，而不是减慢数据科学家的速度。这个决定通常是针对您的团队组成和业务需求的。如果您的团队确实需要低代码/无代码的数据科学工具，那么从像 KNIME Analytics Platform 这样的开源选项开始通常是一种谨慎的方法——基于其采用和有用性，可以考虑更昂贵和更高级的选项。

## 数据科学应用/工作流

人们通常认为数据科学团队是在孤岛中运作的。这种感觉通常是因为数据科学团队公开的主要成果是在数据科学团队之外无法轻松使用的模型，例如，当软件工程师甚至需要在模型之上构建原型应用程序来展示模型的功能时。

这种情况的主要原因是技术堆栈的不匹配——虽然数据科学家使用 R 和 Python 等语言，但创建原型应用程序来展示模型需要 JavaScript、web 服务器、REST APIs 等经典 web 应用程序技能。为了解决这个问题，我们在当今的行业中看到了几种方法 <sup>[10](#Fn10)</sup> :

*   使数据科学家或数据工程师能够使用他们熟悉的技术快速创建能够调用其模型/脚本的原型应用。Plotly Dash 和 Bokeh 就是这样的例子——它们允许只使用 Python 代码快速构建 web 应用程序的原型。 <sup>[11](#Fn11)</sup> 因此，数据科学家/工程师创建的任何模型/可视化都可以直接插入，以快速创建交互式 web 应用。

*   对于使用可视化工作流工具的数据科学团队，可以使用引导式分析工作流来公开模型。数据科学家/工程师创建的这些工作流可以使用用户在 web 应用程序上的输入来交互执行。KNIME 服务器就是一个例子。

这些方法使数据科学团队能够将其他利益相关者、数据分析师和公民数据科学家纳入数据科学流程。

## 现成的人工智能服务和图书馆

有几个人工智能服务和库面向特定类别的问题:

*   在某些情况下，比如时间序列预测或推荐系统，模型取决于你的特定数据——所以这些服务使用你的数据训练模型，然后提供用于推理的 API。

*   在其他情况下，例如与处理图像、文本、语音等相关的问题。，这些服务直接提供了用于推断的 API，因为它们已经使用自己的数据训练了模型。这是因为，例如，文本的情感或人脸的情感普遍适用于任何数据集。这些库/服务通常也允许使用您的训练数据来定制模型。

表 [18-1](#Tab1) 中提到了一些流行的人工智能服务和库。

表 18-1

人工智能服务/库的例子

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

问题类别

 | 

服务/图书馆示例

 |
| --- | --- |
| 时间数列预测法 | 先知，亚马逊预测 |
| 推荐系统 | 亚马逊个性化，谷歌推荐 |
| 自然语言处理 | 亚马逊理解，Azure 文本分析，谷歌自然语言 |
| 计算机视觉 | Amazon Rekognition、Azure 计算机视觉、Azure 媒体视频分析器 |
| 语音处理 | 亚马逊转录/Polly、Azure 认知语音服务、谷歌语音转文本 |

除此之外，还可以找到许多特定领域的库，如医学成像等。通常情况下，您会希望了解您的领域中是否存在这样的库/服务——最好是开源的——供您利用。

有效使用这些 AI 服务和库所需的技能与使用 AutoML 所需的技能类似。因此，ML 工程师——我们在前面的 AutoML 部分遇到过——通常也是利用 AI 服务/库的理想人选。

### 何时使用

如果你正在构建一个涉及到表 [18-1](#Tab1) 中所包含的一类问题的应用程序，谨慎的做法可能是从使用一个相应的服务开始。与您自己从头开始构建模型相比，这可能需要更少的投资。它还可以加快您的上市时间，让您专注于将应用程序的基本功能提供给用户。如果您希望使用前面提到的一些功能来自动化您组织中的一些流程，这同样适用。

虽然我们可以在很大程度上依赖这些服务，因为它们被几个组织使用，但不能保证它在您的特定业务和领域中的表现如何。因此，在任何情况下批判性地评估模型的性能都是很重要的。

如果您的产品或应用程序旨在通过利用您自己的领域专业知识和数据在市场中脱颖而出，那么创建您自己的模型是有意义的。在这种情况下，使用前面提到的服务作为基线基准仍然是有用的。

对于云服务来说，最后一个需要考虑的方面是安全性。阅读细则很重要——一些云服务可能会使用您的数据来不断改进他们的服务，您可能需要明确选择退出。

## 开源与付费

正如我们在前面章节中看到的，数据科学领域很大程度上基于开源库和框架。

虽然付费工具可以提供各种好处，但不要被某个工具或供应商所束缚，这对数据科学来说尤其重要。这样做会降低运营的灵活性，还可能限制你招聘的人才库。

选择一个构建在开源技术之上的工具通常是一个很好的折中方案。这使您能够限制供应商锁定的程度，同时使用付费工具的功能来提高生产率。Databricks 平台就是一个做得非常好的工具。

*   Databricks 笔记本可以轻松地从开源 Jupyter 笔记本导出/导入。

*   Databricks 计算集群基于开源的 Spark。
    *   Databricks 查询引擎使用开源的 SparkSQL。

*   Databricks 集群预装了流行的开源 ML 库。

*   Databricks Delta 基于开源存储层 Delta Lake。

*   Databricks 中的实验和模型注册基于开源的 MLFlow。

各种其他工具在不同程度上遵循类似的开源策略。如果您决定选择没有开源战略的付费工具，那么这是一个重大的战略决策，需要强有力的支持，具体到您的业务和数据科学团队。

## 结论

在本章中，我们看到了在数据科学流程的多个步骤中使用的各类工具。

在这一章中，我们总结了技巧、工具和技术。在下一章中，我们将看到一个参考架构，它说明了到目前为止，第 3 部分中讨论的各种技术是如何在执行整个数据科学过程中结合在一起的。

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

集成开发环境。

  [2](#Fn2_source)

我们在第 17 章中讨论了模型注册。

  [3](#Fn3_source)

例如，Databricks 将 MLFlow 集成到其环境中。

  [4](#Fn4_source)

包含在第 [15 章](15.html)中。

  [5](#Fn5_source)

包含在第 [13 章](13.html)中。

  [6](#Fn6_source)

弹性集装箱服务。

  [7](#Fn7_source)

搜索神经网络的各种结构以确定最佳结构被称为神经结构搜索或 NAS。我们在第 [10](10.html) 章看到的 EfficientNet 系列型号是 NAS 的成果。

  [8](#Fn8_source)

这个术语是 Forrester 创造的。

  [9](#Fn9_source)

参见第 [21](21.html) 章了解公民数据科学家的角色。

  [10](#Fn10_source)

如果您碰巧正在使用一个高级工具，比如 SAS VDMML，那么这已经很容易得到了。所以，你不需要这些方法。

  [11](#Fn11_source)

R 生态系统中的对等物是闪亮的。

 </aside>